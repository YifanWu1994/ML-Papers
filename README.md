# NLP Notes
 
### Attention

>**Add/concat Attention**
>>Resources: [[Paper]](https://github.com/ywu94/NLP-Notes/blob/master/Papers/Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate.pdf),&nbsp; [[Implementation in TF2.0]](https://github.com/ywu94/NLP-Notes/blob/master/Implementations/add-attn-tf2implementation.py),&nbsp; [[Illustrative Introduction]](https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3)

>**Multiplicative Attention**
>>Resources: [[Paper]](https://github.com/ywu94/NLP-Notes/blob/master/Papers/Effective-Approaches-to-Attention-based-Neural-Machine-Translation.pdf),&nbsp; [[Implementation in TF2.0]](https://github.com/ywu94/NLP-Notes/blob/master/Implementations/mul-attn-tf2implementation.py),&nbsp; [[Illustrative Introduction]](https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3) 

>**Multi-head Self Attention**
>>Resources: [[Paper]](https://github.com/ywu94/NLP-Notes/blob/master/Papers/Attention-Is-All-You-Need.pdf),&nbsp; [[Implementation in TF2.0]](https://github.com/ywu94/NLP-Notes/blob/master/Implementations/transformer-tf2implementation.py)
   
### Industrial Application

>**Google Neural Machine Translation System**
>>Concept applied: `Add/concat attention`, `Residual connection` 

>>Resources: [[Paper]](https://github.com/ywu94/NLP-Notes/blob/master/Papers/Google%E2%80%99s-Neural-Machine-Translation-System.pdf),&nbsp; [[Implementation in TF2.0]](https://github.com/ywu94/NLP-Notes/blob/master/Implementations/gnmt-tf2implementation.py),&nbsp; [[Illustrative Introduction]](https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3) 
   
### RNN Tricks
>**Dropout**
>>Trick: Set dropout probability of RNN to `0.2`

>>Resources: [[Paper]](https://github.com/ywu94/NLP-Notes/blob/master/Papers/Recurrent-Neural-Network-Regularization.pdf)

