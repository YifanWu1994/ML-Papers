# NLP Notes
 
### Attention

>**Additive/concat Attention**
>>Resources: [[Paper]](https://github.com/ywu94/NLP-Notes/blob/master/Papers/Neural-Machine-Translation-by-Jointly-Learning-to-Align-and-Translate.pdf),&nbsp; [[Illustrative Intro]](https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3),&nbsp; [[TF2 Implementation]](https://github.com/ywu94/NLP-Notes/blob/master/Implementations/add-attn-tf2implementation.py)

>**Multiplicative Attention**
>>Resources: [[Paper]](https://github.com/ywu94/NLP-Notes/blob/master/Papers/Effective-Approaches-to-Attention-based-Neural-Machine-Translation.pdf),&nbsp; [[Illustrative Intro]](https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3) ,&nbsp; [[TF2 Implementation]](https://github.com/ywu94/NLP-Notes/blob/master/Implementations/mul-attn-tf2implementation.py)

>**Multi-head Self Attention / Transformer**
>>Resources: [[Paper]](https://github.com/ywu94/NLP-Notes/blob/master/Papers/Attention-Is-All-You-Need.pdf),&nbsp; [[Illustrative Intro]](http://jalammar.github.io/illustrated-transformer/) ,&nbsp; [[TF2 Implementation]](https://github.com/ywu94/NLP-Notes/blob/master/Implementations/transformer-tf2implementation.py),&nbsp;
[[Official TF2 Implementation]](https://www.tensorflow.org/tutorials/text/transformer)


   
### Industrial Application

>**Google Neural Machine Translation System**
>>Concept applied: `Additive/concat attention`, `Residual connection` <br/>Resources: [[Paper]](https://github.com/ywu94/NLP-Notes/blob/master/Papers/Google%E2%80%99s-Neural-Machine-Translation-System.pdf),&nbsp; [[Implementation in TF2.0]](https://github.com/ywu94/NLP-Notes/blob/master/Implementations/gnmt-tf2implementation.py),&nbsp; [[Illustrative Introduction]](https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3) 
   
### RNN Tricks
>**Dropout**
>>Trick: Set dropout probability of RNN to `0.2`<br/>Resources: [[Paper]](https://github.com/ywu94/NLP-Notes/blob/master/Papers/Recurrent-Neural-Network-Regularization.pdf)

